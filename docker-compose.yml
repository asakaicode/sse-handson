services:
  ollama:
    container_name: ollama
    build:
      context: ./ollama
      dockerfile: Dockerfile
    profiles:
      - ollama
    ports:
      - "11434:11434"
    environment:
      - OLLAMA_MODEL=llama3.1:8b
    volumes:
      - ollama:/root/.ollama
    restart: unless-stopped
    # GPU (optional): uncomment for NVIDIA GPUs on Linux
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - capabilities: [gpu]
    # For NVIDIA Container Toolkit, you may also need:
    # environment:
    #   - NVIDIA_VISIBLE_DEVICES=all
    #   - NVIDIA_DRIVER_CAPABILITIES=compute,utility
    networks:
      - sse-net

  backend:
    container_name: backend
    build:
      context: ./backend
      dockerfile: Dockerfile
    ports:
      - "3001:3001"
    environment:
      - PORT=3001
      - OLLAMA_BASE_URL=http://ollama:11434
      - OLLAMA_MODEL=llama3.1:8b
      - ENABLE_OLLAMA=${ENABLE_OLLAMA:-false}
      - FALLBACK=true
    networks:
      - sse-net

  frontend:
    container_name: frontend
    build:
      context: ./frontend
      dockerfile: Dockerfile
    ports:
      - "3000:3000"
    environment:
      - NEXT_PUBLIC_BACKEND_URL=http://backend:3001
    depends_on:
      - backend
    networks:
      - sse-net

networks:
  sse-net:
    driver: bridge

volumes:
  ollama:
    driver: local
